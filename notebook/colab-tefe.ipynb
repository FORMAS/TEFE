{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEFE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI-bFsITylR7"
      },
      "source": [
        "TEFE - TimeBankPT Event Frame Extraction\n",
        "\n",
        "[![Github](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/FORMAS/TEFE)\n",
        "\n",
        "[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://hub.docker.com/r/andersonsacramento/tefe)\n",
        "\n",
        "\n",
        "\n",
        "# DESCRIPTION\n",
        "TEFE is a closed domain event extractor system for sentences in the Portuguese language. It extracts events from sentences, which means that it does event detection (i.e., event trigger identification and classification), and argument role prediction (i.e., argument identification and role classification). The event types are based on the typology of the FrameNet project (BAKER; FILLMORE; LOWE, 1998). The models were trained on an enriched TimeBankPT (COSTA; BRANCO,2012) corpus.\n",
        "\n",
        "\n",
        "Currently, in this Colab, 5 different trained models are available to execution: 0, 100, 0\\_0, 100\\_0, 100\\_100, which respectively correspond to: 514 event types (ET) and 1936 argument roles (AR), 7 ET and 93 AR, 214 ET and 477 AR, 5 ET and 42 AR, and 5 ET and 12 AR.\n",
        "\n",
        "## How to cite this work\n",
        "\n",
        "Peer-reviewed accepted paper:\n",
        "\n",
        "\n",
        "* Sacramento, A., Souza, M.: Joint Event Extraction with Contextualized Word Embeddings for the Portuguese Language. In: 10th Brazilian Conference on Intelligent System, BRACIS, São Paulo, Brazil, from November 29 to December 3, 2021.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq8k4MsB2G-V"
      },
      "source": [
        "# Download and locate BERTimbau Base model and TEFE model files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzXEutZg6zD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a0e716d-14a9-4f56-b174-ba126455194c"
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9scRmD73fms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e50d4a8-835b-46c4-c926-cf599460553a"
      },
      "source": [
        "!gdown --id 1lEhJK2gpD8ep7N3KPtFbNPzOB4gQXZX6 --output tefe.zip\n",
        "!unzip tefe.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lEhJK2gpD8ep7N3KPtFbNPzOB4gQXZX6\n",
            "To: /content/tefe.zip\n",
            "190MB [00:01, 152MB/s]\n",
            "Archive:  tefe.zip\n",
            "   creating: models/\n",
            "  inflating: models/blstmea_0.h5     \n",
            "  inflating: models/blstme_100_100.h5  \n",
            "  inflating: models/blstme_100.h5    \n",
            "  inflating: models/blstmeat2_100_0.h5  \n",
            "  inflating: models/blstme_0_0.h5    \n",
            "   creating: res/\n",
            "  inflating: res/args_by_pos_types_12.json  \n",
            "  inflating: res/args_by_pos_types_477.json  \n",
            "  inflating: res/args_by_pos_types_42.json  \n",
            "  inflating: res/events_by_pos_types_7.json  \n",
            "  inflating: res/events_by_pos_types_514.json  \n",
            "  inflating: res/args_by_pos_types_1936.json  \n",
            "  inflating: res/args_by_pos_types_93.json  \n",
            "  inflating: res/events_by_pos_types_5.json  \n",
            "  inflating: res/events_by_pos_types_214.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhtekE-V05-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7869f8-7897-4ac9-a35b-4ff0ee5d497e"
      },
      "source": [
        "!gdown --id 1qIR2GKpBqB-sOmX0Q5j1EQ6NSugYMCsX --output bertimbau.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qIR2GKpBqB-sOmX0Q5j1EQ6NSugYMCsX\n",
            "To: /content/bertimbau.zip\n",
            "1.21GB [00:07, 158MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWOokIB417MY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69b23c0-427b-42f8-89c9-daf929b39df5"
      },
      "source": [
        "!mv bertimbau.zip models/\n",
        "!unzip models/bertimbau.zip -d models/\n",
        "!rm models/bertimbau.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  models/bertimbau.zip\n",
            "  inflating: models/BERTimbau/bert_model.ckpt.index  \n",
            "  inflating: models/BERTimbau/bert_config.json  \n",
            "  inflating: models/BERTimbau/vocab.txt  \n",
            "  inflating: models/BERTimbau/bert_model.ckpt.meta  \n",
            "  inflating: models/BERTimbau/bert_model.ckpt.data-00000-of-00001  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MPvMP4S1p9p"
      },
      "source": [
        "# Load TEFE code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwXRIkl189sF"
      },
      "source": [
        "## install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCb4V4wp1pKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb2c968-b971-4e0c-9292-71b60f2699f7"
      },
      "source": [
        "!pip install tensorflow>=2.6.0\n",
        "!pip install keras-bert>=0.88\n",
        "!pip install numpy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl0cfZgW9B8x"
      },
      "source": [
        "## load functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23p_KaTw9BWN"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "\n",
        "from keras_bert import load_vocabulary, load_trained_model_from_checkpoint, Tokenizer, get_checkpoint_paths\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "BERTIMBAU_MODEL_PATH = 'models/BERTimbau/'\n",
        "EMBEDDING_ID = 'last_hidden'\n",
        "\n",
        "\n",
        "RUN_CONFIGS = {\n",
        "        '0':       {'model':        'models/blstmea_0.h5',\n",
        "                    'events-types': 'res/events_by_pos_types_514.json',\n",
        "                    'args-types':   'res/args_by_pos_types_1936.json'},\n",
        "        '100':     {'model':        'models/blstme_100.h5',\n",
        "                    'events-types': 'res/events_by_pos_types_7.json',\n",
        "                    'args-types':   'res/args_by_pos_types_93.json'},\n",
        "        '0-0':     {'model':        'models/blstme_0_0.h5',\n",
        "                    'events-types': 'res/events_by_pos_types_214.json',\n",
        "                    'args-types':   'res/args_by_pos_types_477.json'},\n",
        "        '100-0':   {'model':        'models/blstmeat2_100_0.h5',\n",
        "                    'events-types': 'res/events_by_pos_types_5.json',\n",
        "                    'args-types':   'res/args_by_pos_types_42.json'},\n",
        "        '100-100': {'model':        'models/blstme_100_100.h5',\n",
        "                    'events-types': 'res/events_by_pos_types_5.json',\n",
        "                    'args-types':   'res/args_by_pos_types_12.json'}}\n",
        "\n",
        "DEFAULT_RUN_CONFIG = '100'\n",
        "\n",
        "def tokenize_and_compose(text):\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        text_tokens = []\n",
        "        for i, token in enumerate(tokens):\n",
        "            split_token = token.split(\"##\")\n",
        "            if len(split_token) > 1:\n",
        "                token = split_token[1]\n",
        "                text_tokens[-1] += token\n",
        "            else:\n",
        "                text_tokens.append(token)\n",
        "        if len(text_tokens[1:-1]) == 1:\n",
        "          return text_tokens[1]\n",
        "        else:\n",
        "          return text_tokens[1:-1]\n",
        "\n",
        "\n",
        "def compose_token_embeddings(sentence, tokenized_text, embeddings):\n",
        "        tokens_indices_composed = [0] * len(tokenized_text)\n",
        "        j = -1\n",
        "        for i, x in enumerate(tokenized_text):\n",
        "            if x.find('##') == -1:\n",
        "                j += 1\n",
        "            tokens_indices_composed[i] = j\n",
        "        word_embeddings = [0] * len(set(tokens_indices_composed))\n",
        "        j = 0\n",
        "        for i, embedding in enumerate(embeddings):\n",
        "            if j == tokens_indices_composed[i]:\n",
        "                word_embeddings[j] = embedding\n",
        "                j += 1\n",
        "            else:\n",
        "                word_embeddings[j - 1] += embedding\n",
        "        return word_embeddings\n",
        "\n",
        "def extract(text, options={'sum_all_12':True}, seq_len=512, output_layer_num=12):\n",
        "        features = {k:v for (k,v) in options.items() if v}\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        indices, segments = tokenizer.encode(first = text, max_len = seq_len)\n",
        "        predicts = model_bert.predict([np.array([indices]), np.array([segments])])[0]\n",
        "        predicts = predicts[1:len(tokens)-1,:].reshape((len(tokens)-2, output_layer_num, 768))\n",
        "\n",
        "        for (k,v) in features.items():\n",
        "            if k == 'sum_all_12':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts.sum(axis=1))\n",
        "            if k == 'sum_last_4':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts[:,-4:,:].sum(axis=1))\n",
        "            if k == 'concat_last_4':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts[:,-4:,:].reshape((len(tokens)-2,768*4)))\n",
        "            if k == 'last_hidden':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts[:,-1:,:].reshape((len(tokens)-2, 768)))\n",
        "        return features\n",
        "\n",
        "\n",
        "\n",
        "def get_sentence_original_tokens(sentence, tokens):\n",
        "        token_index = 0\n",
        "        started = False\n",
        "        sentence_pos_tokens = []\n",
        "        i = 0\n",
        "        while i < len(sentence):\n",
        "                if sentence[i] != ' ' and not started:\n",
        "                        start = i\n",
        "                        started = True\n",
        "                if sentence[i] == tokens[token_index] and started:\n",
        "                        sentence_pos_tokens.append(sentence[i])\n",
        "                        started = False\n",
        "                        token_index += 1\n",
        "                elif i<len(sentence) and (sentence[i] == ' ' or tokenize_and_compose(sentence[start:i+1]) == tokens[token_index] ) and started:\n",
        "                        sentence_pos_tokens.append(sentence[start:i+1])\n",
        "                        start = i+1\n",
        "                        started = False\n",
        "                        token_index += 1\n",
        "                i += 1\n",
        "        return sentence_pos_tokens\n",
        "\n",
        "\n",
        "def get_text_location(text, arg, start_search_at=0):\n",
        "        text = text.lower()\n",
        "        arg = arg.lower()\n",
        "        pattern = re.compile(r'\\b%s\\b' % arg)\n",
        "        match = pattern.search(text, start_search_at)\n",
        "        if match:\n",
        "                return (match.start(), match.end())\n",
        "        else:\n",
        "                return (-1, -1)\n",
        "\n",
        "\n",
        "def get_args_from_labels(label_args, is_arp=True):\n",
        "        args = []\n",
        "        cur_arg = []\n",
        "        started_arg = False\n",
        "        fn_normalize_label = lambda cur_label : cur_label[-1] if cur_label[-1] <= len(args_types) else cur_label[-1] - len(args_types)\n",
        "        for i,label in enumerate(label_args):\n",
        "                if not started_arg and label != 0 and label <= len(args_types):\n",
        "                        cur_arg.append((i, label if is_arp else 1))\n",
        "                        started_arg = True\n",
        "                elif started_arg and label != 0 and label > len(args_types):\n",
        "                        last_label = fn_normalize_label(cur_arg[-1])\n",
        "                        if label-len(args_types) != last_label and is_arp:\n",
        "                                cur_arg = []\n",
        "                                started_arg = False\n",
        "                        else:\n",
        "                                cur_arg.append((i,label if is_arp else 1))\n",
        "                elif started_arg and label == 0:\n",
        "                        args.append(tuple(cur_arg))\n",
        "                        cur_arg = []\n",
        "                        started_arg = False\n",
        "                elif started_arg and  label <= len(args_types):\n",
        "                        args.append(tuple(cur_arg))\n",
        "                        cur_arg = []\n",
        "                        cur_arg.append((i, label if is_arp else 1))\n",
        "                        started_arg = True\n",
        "        if cur_arg:\n",
        "                args.append(tuple(cur_arg))\n",
        "        return args\n",
        "\n",
        "\n",
        "def extract_events(text, feature_option, is_pprint=True):\n",
        "        text_tokens = get_sentence_original_tokens(text, tokenize_and_compose(text))\n",
        "        features = extract(text, {feature_option:True})[feature_option]\n",
        "        embeddings = np.array(features).reshape((len(text_tokens), 768))\n",
        "        sentence_embeddings = np.zeros((1,128,768))\n",
        "        sentence_embeddings[0,:len(text_tokens)] = embeddings\n",
        "        predictions = [model.predict([e.reshape((1, 768)), sentence_embeddings]) for e in embeddings]\n",
        "        positions = list(filter((lambda i: i>= 0 and i < len(text_tokens)), [pos for (pos, (pred_ed, pred_args)) in enumerate(predictions) if np.argmax(pred_ed) != 0]))\n",
        "        output = []\n",
        "        if len(positions) > 0:\n",
        "                start_at = sum([len(token) for token in text_tokens[:positions[0]]])\n",
        "        for pos in positions:\n",
        "                loc_start, loc_end = get_text_location(text, text_tokens[pos], start_at)\n",
        "                start_at = loc_end\n",
        "                args_preds =  [np.argmax(predictions[pos][1][0,i,:]) for i in range(predictions[pos][1].shape[1]) if i < len(text_tokens)]\n",
        "                start_arg_search = 0\n",
        "                args_event = []\n",
        "                event_type = events_types[str(np.argmax(predictions[pos][0]))]\n",
        "                for arg_tokens in get_args_from_labels(args_preds):\n",
        "                        first_arg_token = arg_tokens[0]\n",
        "                        last_arg_token = arg_tokens[-1]\n",
        "                        try:\n",
        "                          pattern = re.compile(r'\\b%s\\b' % '\\s*'.join([text_tokens[arg_token[0]] for arg_token in arg_tokens]))\n",
        "                        except:\n",
        "                          if is_pprint:\n",
        "                            return json.dumps(output, indent=4)\n",
        "                          return output\n",
        "                        match = pattern.search(text, start_arg_search)\n",
        "                        if match:\n",
        "                                arg_type = args_types[str(first_arg_token[1])]\n",
        "                                if str(arg_type['id']) in event_type['args']:\n",
        "                                        args_event.append({'role':arg_type['name'],\n",
        "                                                           'text': text[match.start():match.end()],\n",
        "                                                           'start': match.start(),\n",
        "                                                           'end': match.end()\n",
        "                                                           })\n",
        "                           \n",
        "                                start_arg_search = match.end()\n",
        "                output.append({'trigger':{\n",
        "                        'text': text[loc_start:loc_end],\n",
        "                        'start': loc_start,\n",
        "                        'end' : loc_end},\n",
        "                               'arguments':args_event,\n",
        "                               'event_type': event_type['name']\n",
        "                               })\n",
        "        if is_pprint:\n",
        "          return json.dumps(output, indent=4)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def load_bertimbau_model():    \n",
        "        global tokenizer\n",
        "        global model_bert\n",
        "        \n",
        "        paths = get_checkpoint_paths(BERTIMBAU_MODEL_PATH)\n",
        "\n",
        "        model_bert = load_trained_model_from_checkpoint(paths.config, paths.checkpoint, seq_len=512, output_layer_num=12)\n",
        "\n",
        "        token_dict = load_vocabulary(paths.vocab)\n",
        "        tokenizer = Tokenizer(token_dict)\n",
        "\n",
        "def load_tefe_model():\n",
        "        global model\n",
        "        global events_types\n",
        "        global args_types\n",
        "\n",
        "        events_types, args_types = load_events_args_info()\n",
        "        model = load_model(RUN_CONFIGS[model_config]['model'])\n",
        "        return model\n",
        "\n",
        "def load_events_args_info():\n",
        "        events_types, args_types = {}, {}\n",
        "\n",
        "        with open(RUN_CONFIGS[model_config]['events-types'], 'r') as read_content:        \n",
        "                events_types = json.load(read_content)\n",
        "                \n",
        "        with open(RUN_CONFIGS[model_config]['args-types'], 'r') as read_content:        \n",
        "                args_types = json.load(read_content)                \n",
        "\n",
        "        return events_types, args_types\n",
        "\n",
        "\n",
        "\n",
        "def extract_from_files(input_path, output_path):\n",
        "        for filepathname in glob.glob(f'{input_path}*.txt'):\n",
        "                extractions = []\n",
        "                for line in open(filepathname):\n",
        "                        line = line.strip()\n",
        "                        print(line)\n",
        "                        extractions.append(extract_events(line, EMBEDDING_ID))\n",
        "\n",
        "                filename = filepathname.split('.txt')[0].split(os.sep)[-1]\n",
        "                with open(f'{output_path}{filename}.json', 'w')  as outfile:\n",
        "                        json.dump(extractions, outfile)\n",
        "                print(f'{filename}')\n",
        "\n",
        "\n",
        "def extract_events_from(input_path, output_path):\n",
        "        run_extraction_context(lambda : extract_from_files(input_path, output_path))\n",
        "        \n",
        "\n",
        "def extract_events_from_sentence(sentence):\n",
        "        sentence = sentence.strip()\n",
        "        run_extraction_context(lambda : print(extract_events(sentence, EMBEDDING_ID)))\n",
        "        \n",
        "\n",
        "def run_extraction_context(run_extraction_func):                        \n",
        "        if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "                with tf.device('/GPU:0'):\n",
        "                        load_bertimbau_model()\n",
        "                        load_tefe_model()\n",
        "                        run_extraction_func()\n",
        "        else:\n",
        "                with tf.device('/cpu:0'):\n",
        "                        load_bertimbau_model()\n",
        "                        load_tefe_model()\n",
        "                        run_extraction_func()\n",
        "                        \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvCGh-Uu9zAB"
      },
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92lDPSbg90rc"
      },
      "source": [
        "## Extract Events From Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l9mlzjM9pw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765edc35-98e8-4232-cf5a-ecb4e324f073"
      },
      "source": [
        "#@title Input the sentence and select the model\n",
        "\n",
        "sentence = 'A Petrobras aumentou o preço da gasolina para 2,30 reais, disse o presidente.' #@param {type:\"string\"}\n",
        "model_config = '100' #@param [\"0\", \"100\", \"0-0\", \"100-0\", \"100-100\"]\n",
        "\n",
        "\n",
        "print(sentence)\n",
        "print(model_config)\n",
        "extract_events_from_sentence(sentence)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Petrobras aumentou o preço da gasolina para 2,30 reais, disse o presidente.\n",
            "100\n",
            "[\n",
            "    {\n",
            "        \"trigger\": {\n",
            "            \"text\": \"aumentou\",\n",
            "            \"start\": 12,\n",
            "            \"end\": 20\n",
            "        },\n",
            "        \"arguments\": [\n",
            "            {\n",
            "                \"role\": \"Cause_change_of_position_on_a_scale#Agent\",\n",
            "                \"text\": \"A Petrobras\",\n",
            "                \"start\": 0,\n",
            "                \"end\": 11\n",
            "            },\n",
            "            {\n",
            "                \"role\": \"Cause_change_of_position_on_a_scale#Attribute\",\n",
            "                \"text\": \"o pre\\u00e7o da\",\n",
            "                \"start\": 21,\n",
            "                \"end\": 31\n",
            "            },\n",
            "            {\n",
            "                \"role\": \"Cause_change_of_position_on_a_scale#Item\",\n",
            "                \"text\": \"gasolina\",\n",
            "                \"start\": 32,\n",
            "                \"end\": 40\n",
            "            },\n",
            "            {\n",
            "                \"role\": \"Cause_change_of_position_on_a_scale#Value_2\",\n",
            "                \"text\": \"2,30 reais\",\n",
            "                \"start\": 46,\n",
            "                \"end\": 56\n",
            "            }\n",
            "        ],\n",
            "        \"event_type\": \"Cause_change_of_position_on_a_scale\"\n",
            "    },\n",
            "    {\n",
            "        \"trigger\": {\n",
            "            \"text\": \"disse\",\n",
            "            \"start\": 58,\n",
            "            \"end\": 63\n",
            "        },\n",
            "        \"arguments\": [\n",
            "            {\n",
            "                \"role\": \"Statement#Message\",\n",
            "                \"text\": \"A Petrobras aumentou o pre\\u00e7o da gasolina para 2,30 reais\",\n",
            "                \"start\": 0,\n",
            "                \"end\": 56\n",
            "            },\n",
            "            {\n",
            "                \"role\": \"Statement#Speaker\",\n",
            "                \"text\": \"o presidente\",\n",
            "                \"start\": 64,\n",
            "                \"end\": 76\n",
            "            }\n",
            "        ],\n",
            "        \"event_type\": \"Statement\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJLs3Gtm95Cl"
      },
      "source": [
        "## Extract Events From Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX54lpbhFc3K"
      },
      "source": [
        "# If you want to be able to process files from your drive folders \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joAsjR1BFoJc"
      },
      "source": [
        "#@title ## Input and Output directory fields\n",
        "\n",
        "#@markdown The text files in the input directory are expected to have the format:\n",
        "\n",
        "#@markdown * all text files end with the extension .txt\n",
        "#@markdown * sentences are separated by newlines\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Enter the directories paths:\n",
        "input_dir = \"/content/drive/MyDrive/input-files/\" #@param {type:\"string\"}\n",
        "output_dir = \"/content/drive/MyDrive/output-files/\" #@param {type:\"string\"}\n",
        "model_config = '100' #@param [\"0\", \"100\", \"0-0\", \"100-0\", \"100-100\"]\n",
        "#@markdown ---\n",
        "\n",
        "extract_events_from(input_dir, output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}